# ğŸ§  Arabic RAG Question Answering System (FAISS + BGE-M3 + Gemini)

An end-to-end **Retrieval Augmented Generation (RAG)** system designed to answer Arabic questions using document-grounded knowledge instead of relying only on LLM memory.

The system performs semantic retrieval using vector embeddings and generates reliable answers using **Google Gemini LLM**.

---

## ğŸš€ Overview

This project demonstrates how modern AI systems combine:

* Semantic Embeddings
* Vector Databases
* Context Retrieval
* Large Language Models (LLMs)

to build accurate and explainable Question Answering systems.

Instead of hallucinating answers, the model retrieves relevant document chunks before generating responses.

---

## âœ¨ Features

âœ… Arabic document understanding
âœ… Semantic similarity search using FAISS
âœ… High-quality multilingual embeddings (BAAI BGE-M3)
âœ… Chunking with overlap for context preservation
âœ… Context-grounded Gemini LLM answers
âœ… Interactive command-line chat loop
âœ… Secure API key handling using `.env`

---

## ğŸ—ï¸ Architecture

```
Document (arabic.txt)
        â†“
Text Chunking + Overlap
        â†“
BGE-M3 Embeddings
        â†“
FAISS Vector Index
        â†“
User Question
        â†“
Semantic Similarity Search
        â†“
Top-K Relevant Chunks
        â†“
Prompt Construction
        â†“
Gemini LLM
        â†“
Final Answer
```

---

## ğŸ§  Technologies Used

* Python
* Sentence Transformers
* BAAI BGE-M3 Embeddings
* FAISS Vector Database
* Google Gemini API
* NumPy
* python-dotenv

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py        # Main RAG pipeline
â”œâ”€â”€ arabic.txt     # Arabic knowledge base
â”œâ”€â”€ english.txt    # Optional dataset
â”œâ”€â”€ .env           # API keys (ignored in git)
â””â”€â”€ README.md
```

---

## ğŸ”§ Installation

Clone repository:

```
git clone https://github.com/YOUR-USERNAME/YOUR-REPO.git
cd YOUR-REPO
```

Create virtual environment:

```
python -m venv venv
```

Activate environment:

Windows:

```
venv\Scripts\activate
```

Install dependencies:

```
pip install -r requirements.txt
```

---

## ğŸ”‘ Environment Variables

Create a `.env` file in project root:

```
GOOGLE_API_KEY=your_gemini_api_key
```

---

## â–¶ï¸ Run The Project

```
python main.py
```

Interactive prompt will appear:

```
Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ:
```

Type:

```
exit
```

to close the application.

---

## ğŸ” Retrieval Workflow

1. Documents are split into overlapping chunks.
2. Each chunk is converted into embeddings using BGE-M3.
3. FAISS builds a similarity search index.
4. User question embedding retrieves nearest chunks.
5. Retrieved context is injected into Gemini prompt.

This significantly reduces hallucinations and improves accuracy.

---

## ğŸ“Š Example Usage

Question:

```
Ù…Ø§ Ù‡Ùˆ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ
```

System Output:

```
Chunks retrieved:
- ...

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:
...
```

---

## ğŸ”’ Safety

The system instructs the LLM to:

* Answer only using retrieved context.
* Respond with "Ù„Ø§ Ø£Ø¹Ø±Ù" if the answer is unavailable.

---

## ğŸ“ˆ Future Improvements

* Streamlit Web Interface
* Multi-document ingestion
* Hybrid Search (BM25 + Vector)
* Persistent FAISS index saving
* Source citation display

---

## ğŸ‘¨â€ğŸ’» Author

**Ebraam Nabil**

GitHub:
https://github.com/EbraamNabil

---

## â­ Purpose

This project demonstrates practical skills in:

* Retrieval Augmented Generation (RAG)
* NLP Engineering
* Vector Databases
* LLM Application Development

Designed for portfolio presentation, internships, and AI engineering roles.
